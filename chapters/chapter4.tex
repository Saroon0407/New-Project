\chapter{DESIGN AND METHODOLOGY}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}
{\setstretch{1.5}

  This chapter presents the technical design and experimental methodology to be used for developing and evaluating the proposed emergency response system. It outlines the research objectives, measurement parameters, system architecture, UML diagrams, experimental procedures, error considerations with mitigation strategies, and the data analysis techniques to be applied for real-world performance evaluation.
  % -----------------------------------------------------------------------------
  \section{Objective of the Study}

  The primary objective of this study is to design, implement, and evaluate an intelligent emergency response system that enhances response time and user guidance during emergencies. The work aims to develop a reliable mobile solution integrating real-time decision support, communication features, and AR-based first-aid assistance. The research objectives include:
  \begin{itemize}
    \item To develop a rapid SOS trigger mechanism capable of initiating alerts within 3 seconds using optimized UI interactions and event handling.
    \item To implement a hybrid AI-based emergency classification model combining cloud inference with rule-based fallback to achieve over 85\% detection accuracy.
    \item To design an AR-based first-aid assistance module with under 2-second startup time providing step-wise CPR, AED, and bleeding control guidance.
    \item To develop a BLE-based offline communication system targeting over 80\% successful delivery across a cumulative 100-meter mesh relay range.
    \item To analyze and reduce end-to-end response latency including GPS acquisition, network delays, server processing, and multi-channel notification dispatch.
    \item To evaluate system resource consumption covering battery usage, CPU load, memory usage, and network bandwidth across multiple device variants.
    \item To validate usability and reliability through controlled field tests simulating real-world emergencies with diverse environmental conditions.
  \end{itemize}

  % -----------------------------------------------------------------------------
  \section{Measurement Parameters}

  The system performance will be evaluated using the following quantitative parameters:

  \subsection{Latency Metrics}
  \begin{itemize}
    \item \textbf{SOS Dispatch Time ($T_{dispatch}$):} Time elapsed from SOS button press to alert reception by emergency contacts, measured in milliseconds. Target: $T_{dispatch} < 3000$ ms under normal network conditions.
    \item \textbf{AR Initialization Latency ($T_{AR}$):} Time required from AR module invocation to display of first visual overlay, measured in milliseconds. Target: $T_{AR} < 2000$ ms.
    \item \textbf{GPS Acquisition Time ($T_{GPS}$):} Time from location request to first valid coordinate fix, measured in milliseconds.
    \item \textbf{Network Transmission Time ($T_{network}$):} Time for data packet transmission from client to server acknowledgment, measured in milliseconds.
  \end{itemize}

  \subsection{Accuracy Metrics}
  \begin{itemize}
    \item \textbf{AI Classification Accuracy ($A_{class}$):} Percentage of correct emergency type identifications, calculated as:
      \begin{equation}
        A_{class} = \frac{\mathrm{Correct\ Classifications}}{\mathrm{Total\ Test\ Cases}} \times 100\%
      \end{equation}
      Target: $A_{class} > 85\%$
    \item \textbf{Severity Assessment Accuracy ($A_{severity}$):} Percentage of correct severity level assignments (high/moderate/low). Target: $A_{severity} > 80\%$
    \item \textbf{GPS Location Accuracy ($\epsilon_{GPS}$):} Horizontal position error in meters compared to ground truth coordinates. Target: $\epsilon_{GPS} < 10$ meters under open sky.
  \end{itemize}

  \subsection{Reliability Metrics}
  \begin{itemize}
    \item \textbf{BLE Relay Success Rate ($R_{BLE}$):} Percentage of messages successfully delivered through offline relay, calculated as:
      \begin{equation}
        R_{BLE} = \frac{\text{Successfully Delivered Messages}}{\text{Total Broadcast Messages}} \times 100\%
      \end{equation}

      Target: $R_{BLE} > 80\%$ within 100\,m range.

    \item \textbf{Notification Delivery Rate ($R_{notify}$):} Percentage of alerts successfully received by emergency contacts across all channels.
  \end{itemize}

  \subsection{Resource Utilization Metrics}

\begin{itemize}
  \item \textbf{Battery Consumption ($P_{battery}$):} Power draw during active operation including GPS polling, BLE scanning, and network transmission, measured in milliampere-hours (mAh) per hour. Target: $P_{battery} < 150$ mAh/hour during continuous emergency mode.
  \item \textbf{Network Bandwidth ($B_{network}$):} Total data transmitted per SOS event including location payload, user metadata, and encryption overhead, measured in kilobytes (KB). Target: $B_{network} < 50$ KB per alert transmission. 
  \item \textbf{CPU Utilization ($U_{CPU}$):} Average processor usage during AR rendering and AI processing, measured as percentage of total available processing capacity.  Target: $U_{CPU} < 60\%$ to prevent device overheating and ensure smooth operation.
  \item \textbf{Memory Footprint ($M_{app}$):} Application memory consumption during runtime including cached assets and background services, measured in megabytes (MB). Target: $M_{app} < 200$ MB to ensure compatibility with low-end devices. 
\end{itemize}

  % -----------------------------------------------------------------------------
\section{Measurement Method}

 The measurement methodology defines the systematic approach for collecting, recording, and validating performance data across all evaluation parameters. This section describes the prototype deployment strategy, instrumentation techniques, and standardized testing protocols that will ensure reproducible and statistically significant results. 

\subsection{Prototype Deployment}
  A functional prototype of the ResQNow system will be deployed on multiple Android devices (Google Pixel 6, OnePlus 9, Samsung Galaxy S21) running Android versions 11 to 13, implementing all core functionalities including SOS activation, GPS tracking, AI-based emergency classification, AR-guided assistance, BLE-based offline relay, and multi-channel notifications.

  \subsection{Latency Measurement}
  High-precision timestamps with microsecond resolution will be recorded at critical execution points: $t_0$ (SOS activation), $t_1$ (GPS acquisition), $t_2$ (network transmission start), $t_3$ (server reception), $t_5$ (notification dispatch), and $t_6$ (alert reception).
  End-to-end latency will be computed as $T_{dispatch} = t_6 - t_0$, with component-level latencies derived accordingly. Timestamp synchronization will be maintained using Network Time Protocol (NTP).

  \subsection{AI Classification Evaluation}
  A dataset of 100 emergency scenarios with expert-verified ground truth labels will be used to evaluate classification performance. System outputs will be compared against reference labels using confusion matrices, with accuracy, precision, recall, and F1-score computed for each emergency category.

  \subsection{AR Performance Evaluation}
  AR initialization latency and rendering performance will be evaluated using Android Profiler by measuring ARCore initialization time, 3D model loading time, frame rate (FPS), and GPU utilization. All measurements will be averaged across 20 controlled trials.

  \subsection{BLE Relay Evaluation}
  Offline communication performance will be assessed through controlled outdoor experiments using one originating device, multiple relay devices positioned at incremental distances (10m–100m), and one internet-connected gateway device. For each configuration, 50 test messages will be broadcast and the delivery success rate will be recorded.

  \subsection{Resource Utilization Analysis}
  System resource consumption will be profiled using Android Battery Historian for power usage, Charles Proxy for network bandwidth per SOS event, and Android Profiler for CPU and memory utilization during active emergency operation.

  % -----------------------------------------------------------------------------
  \section{System Design Specifications}

  This section presents the detailed system design through architecture diagrams and Unified Modeling Language (UML) diagrams that specify the structural and behavioral aspects of the ResQNow emergency response system.

  \subsection{System Architecture}

  The ResQNow system will follow a three-tier client-server architecture designed for scalability, modularity, and fault tolerance. Each layer will be loosely coupled and communicate through well-defined interfaces and protocols.

  Figure~\ref{fig:system_architecture} illustrates the complete architecture with three major layers. The \textbf{Mobile Application Layer} will run on user devices and include the Flutter app UI, SOS trigger (manual + voice), GPS location capture, AR-based first-aid guidance, and BLE offline relay module for mesh-based communication without internet.

  The \textbf{Backend Services Layer}, to be deployed on Google Cloud Platform, will host the FastAPI backend, Firebase authentication for secure user sessions, AI emergency classification, severity assessment engine, and Firestore database for storing user profiles and incident logs.

  The \textbf{Notification Layer} will enable multi-channel alert delivery via Firebase Cloud Messaging for push notifications, Twilio for SMS alerts, and SendGrid for email reports with location details. Information will flow from the mobile client to backend services and outward to notification channels, using synchronous API calls and asynchronous event triggers. The layered architecture will support independent scalability, service isolation, and graceful degradation during failures.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapters/system_architecture_resqnow.png}
    \caption{Overall System Architecture of the ResQNow Emergency Response System}
    \label{fig:system_architecture}
  \end{figure}
  % -----------------------------------------------------------------------------
 \subsection{Use Case Diagram}

The use case diagram provides a high-level functional specification of the ResQNow system by identifying system actors and their interactions with system capabilities. 

Figure~\ref{fig: use_case_diagram1} presents the complete use case diagram showing three primary actors.  The \textbf{User} (left) interacts through Register/Login, Manage Emergency Contacts, Trigger SOS, Provide AR First-Aid Guidance, and Track Emergency Status use cases. The Capture Location use case has an include relationship with Trigger SOS, indicating mandatory location capture during SOS activation. The Send SOS Alert use case has an extend relationship with Offline SOS Relay, conditionally activating BLE relay when internet is unavailable. 

The \textbf{Emergency Contact} (upper right) receives alert notifications, while the \textbf{Medical Responder} (lower right) can acknowledge and respond to emergencies. The \textbf{ResQNow Backend System} (bottom right) handles alert processing and coordination.  This specification defines 12 functional capabilities and 4 actors guiding subsequent design phases. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.90\textwidth]{chapters/use_case_diagram1.png}
  \caption{Use Case Diagram of the ResQNow System}
  \label{fig:use_case}
\end{figure}

  % -------------------------------------------------------------
\subsection{Activity Diagram for SOS Emergency Response Workflow}

The SOS activation workflow represents the most critical operational sequence in the ResQNow system, executing under time-critical conditions with minimal user burden. Figure~\ref{fig:sos_workflow} depicts the complete SOS emergency response activity flow. 

\textbf{SOS Trigger Method: } The first decision diamond determines how the SOS was initiated.  For Manual SOS (left branch), the user proceeds to Select Severity where they choose the emergency level through UI controls. For AI Chat (right branch), the system analyzes the conversation to automatically determine emergency characteristics. The AI path includes a loop back to Continue Chat if no emergency is detected.  When an emergency is confirmed, the flow proceeds to Raise SOS. 

\textbf{Sequential Processing:} Both manual and AI-assisted paths converge at the Raise SOS action.  The system then executes Capture GPS Location to acquire current coordinates, followed by Broadcast SOS to all users which disseminates the alert including user identification, GPS coordinates, severity level, and timestamp. 

\textbf{Responder Coordination:} Nearby users evaluate the emergency on their devices.  If a responder decides to help, the system executes three parallel actions: Lock responder, Disable response button for others, and Show live location via Google Maps. If no user responds, the SOS remains in active state.  All parallel activities reconverge at the join node before workflow termination.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.2\textwidth]{chapters/UML activity diagram for sos trigger and responce FLOW IN RESQNOW.png}
  \caption{Activity Flow Diagram of Emergency SOS Processing}
  \label{fig:sos_workflow}
\end{figure}

  % -------------------------------------------------------------
 \subsection{Activity Diagram for BLE Offline Relay Workflow}

  The BLE offline relay mechanism addresses a critical vulnerability in emergency alert systems that depend entirely on cellular or internet connectivity. This workflow will enable emergency message propagation through dynamic ad-hoc mesh networks using Bluetooth connectivity between nearby devices.

  Figure~\ref{fig:ble_relay} illustrates the BLE offline relay activity flow showing how encrypted emergency messages will propagate through multi-hop device forwarding.

  \textbf{Trigger and Connectivity Check:} The SOS triggered action will initiate the workflow. If internet connectivity is available, the workflow will terminate immediately. If unavailable, the offline relay mechanism will activate.

  \textbf{Encryption and Broadcasting:} The Encrypt SOS packet action will apply AES-128 symmetric encryption. The Broadcast via BLE action will transmit the encrypted message using BLE advertisements.

  \textbf{Discovery and Relay Loop:} Nearby devices will scan continuously. If no device is found, broadcasting will continue with backoff. If a device is found, the message will be forwarded.

  \textbf{Relay Device Processing:} Devices will increment hop count and re-broadcast until an internet-enabled gateway device is reached.

  \textbf{Confirmation and Termination:} After backend upload, acknowledgment messages will propagate back to the originator.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{chapters/ble based offline sos relay activity flow in resqnow.png}
    \caption{Offline SOS Relay Mechanism Using Bluetooth Low Energy}
    \label{fig:ble_relay}
  \end{figure}

  % -------------------------------------------------------------
\subsection{Activity Diagram for AI-Based Emergency Classification Workflow}

The emergency classification workflow will implement a hybrid artificial intelligence approach combining cloud-based conversational intelligence with rule-based severity mapping to ensure reliable classification across varying network conditions. 

Figure~\ref{fig:ai_classification} presents the rule-based offline emergency classification flowchart showing the fallback mechanism when cloud AI services are unavailable. 

\textbf{Connectivity Decision:} Determines whether cloud AI or offline rules are used. 

\textbf{Keyword Extraction:} Performs preprocessing and symptom keyword identification. 

\textbf{Rule-Based Symptom Matching:} Matches extracted keywords against severity rules.

\textbf{Confidence Scoring:} Computes confidence based on severity weights and keyword frequency.

\textbf{Classification Decision:} Determines severity category (High, Moderate, Low).

\textbf{Severity Assignment:} Triggers Immediate SOS, User Confirmation, or Guidance Only based on classification.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{chapters/Rule-Based offline emergency classification flowchat.png}
  \caption{Rule-Based Offline Emergency Classification Flowchart}
  \label{fig: ai_classification}
\end{figure}

  %-----------------------------------------------------------------------------
  \subsection{Sequence Diagram for Complete SOS Lifecycle}

  Sequence diagrams model the time-ordered interaction between system components, showing precise message exchanges and method invocations during specific scenarios. Unlike activity diagrams that focus on control flow logic, sequence diagrams emphasize inter-object communication and chronological ordering of events.

  Figure~\ref{fig:sequence_diagram} presents the complete SOS lifecycle sequence diagram showing interactions between six system participants arranged horizontally: User (leftmost actor), Mobile App, AI Chatbot, Backend Server, Other App Users (Responders), and Google Maps (rightmost actor). Vertical dashed lines (lifelines) represent each participant's existence over time, with time progressing downward. Thin rectangular boxes on lifelines (activation boxes) indicate periods of active processing.

  \textbf{SOS Initiation Phase:} The sequence will begin with two alternative flows (alt frame labeled "SOS Initiation" at top left). In the [Manual Initiation] scenario (top branch), the User will perform action 1: Initiate SOS (Manual) sending a message to Mobile App, which will execute action 2: Select Severity (e.g., Critical) allowing manual severity selection through UI dropdown. In the [AI Chat Initiation] scenario (bottom branch), the User will perform action 1a: Start Chat \& Provide Symptoms sending conversational text to Mobile App, which will forward the message to AI Chatbot for action 2a: Analyze Conversation \& Symptoms (shown as synchronous call with solid arrow). If the AI detects an emergency, it will perform action 3a: Trigger SOS Automatically (if required) sending a command back to Mobile App. Both branches will converge after the alt frame.

  \textbf{Location Capture:} Following SOS initiation, the Mobile App will send action 4: Request GPS Location to itself (reflexive arrow on Mobile App lifeline), which will return action 5: Return Current Coordinates (dashed arrow indicating response message). This will capture the user's position for alert metadata.

  \textbf{Backend Transmission:} The Mobile App will send action 6: Send SOS Request (Location, Severity, User ID) to Backend Server (long solid arrow crossing to server lifeline) using HTTPS POST request. The server will process the request and perform action 7: Broadcast SOS Alert (Location, Severity) to Other App Users (Responders), shown as a message to the responder lifeline. This notification will appear on nearby users' devices.

  \textbf{Responder Evaluation and Response:} Upon receiving the alert, Other App Users (Responders) will execute action 8: Evaluate Alert (Proximity, Availability) (reflexive arrow) to assess whether they can provide assistance based on distance and current availability. If a responder decides to help, they will send action 9: Click "I'll Respond" (Responder ID) back to Backend Server. The server will execute action 10: Lock SOS \& Assign Responder (dashed arrow indicating internal processing), implementing atomic database transaction to prevent race conditions where multiple responders simultaneously claim the same emergency.

  \textbf{Status Update and Navigation:} After assigning the responder, the Backend Server will send action 11: Disable Response Button \& Show Responder Assigned to Other App Users (Responders), updating UI state for all other potential responders to prevent duplicate acknowledgments. The server will also send action 12: Notify User of Responder Assignment back to Mobile App, informing the victim that help is on the way. The Mobile App will then send action 13: Open Maps with Live Victim Location to itself, which will forward action 14: Open Maps with Live Victim Location (for Responder) to Google Maps (rightmost participant), launching navigation interface showing real-time victim position for responder guidance.

  \textbf{Message Types:} The diagram uses solid arrows for synchronous method calls where the sender waits for response, dashed arrows for asynchronous messages or return values, and reflexive arrows (looping back to same lifeline) for internal processing. The alt frame (alternative fragment) shows mutually exclusive execution paths for manual vs. AI-assisted SOS initiation. Numbering (1, 2, 3a, etc.) indicates chronological ordering of messages.

  This sequence diagram specifies 14 distinct message exchanges, 6 system participants, 2 alternative execution paths (manual and AI-assisted SOS), and the precise ordering of operations from emergency initiation through responder navigation activation. The diagram clarifies that AI chat analysis will occur asynchronously without blocking user interaction, GPS capture will happen locally on the mobile device before backend transmission, backend server will orchestrate responder coordination through database transactions, and status updates will propagate bidirectionally between server and clients. The sequence demonstrates the distributed nature of the system with clear separation between client-side presentation logic (Mobile App), server-side business logic (Backend Server), AI processing (AI Chatbot), and external service integration (Google Maps).

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapters/complete sos lifecycle sequence in resqnow system.png}
    \caption{Sequence Diagram for Complete SOS Lifecycle in ResQNow System}
    \label{fig:sequence_diagram}
  \end{figure}

  % -----------------------------------------------------------------------------
\section{Experimental Procedure}

\textbf{Test Scenario 1: SOS Latency Measurement}

Measure end-to-end SOS dispatch time under normal, degraded, and poor network conditions using traffic shaping.  Press SOS button, record timestamps at initiation ($t_0$) and reception ($t_6$), calculate $T_{dispatch} = t_6 - t_0$.  Repeat for 30 trials per condition with intermediate timestamps for component-level analysis.

\textbf{Test Scenario 2: AI Classification Validation}

Evaluate classification accuracy using 100 prepared emergency descriptions. Submit each to the AI module, record predicted emergency type and severity, compare with ground truth labels.  Calculate accuracy, precision, recall, F1-score, and analyze misclassification patterns.

\textbf{Test Scenario 3: AR Performance Testing}

Measure AR initialization latency and rendering performance across 20 trials.  Record time to first visual overlay, monitor frame rate during 60-second sessions using Android Profiler, and capture CPU/GPU utilization metrics. 

\textbf{Test Scenario 4: BLE Relay Evaluation}

Assess offline message delivery across distances (10m, 30m, 50m, 100m) with gateway at 150m.  Broadcast 50 test messages, record successful deliveries, log RSSI values and hop counts.  Repeat with 2, 3, and 5 relay devices to analyze correlation between distance, node density, and delivery rate.

\textbf{Test Scenario 5: Resource Utilization Profiling}

Execute 1-hour simulated emergency scenario with active GPS, AR, and BLE. Record battery consumption using Android Battery Historian, capture network traffic via Charles Proxy during 10 SOS events, monitor CPU and memory using Android Profiler.  Calculate average bandwidth and peak/average utilization values. 
  % -----------------------------------------------------------------------------

  % -----------------------------------------------------------------------------
  \section{Project Timeline and Development Schedule}

  \subsection{Overview of Development Lifecycle}

  The ResQNow project will be executed following a structured software development lifecycle spanning approximately 11 months from February 2025 to December 2025. The development process will be organized into three major phases: the Planning and Design Phase (February to July 2025) focusing on problem identification, literature review, and system design; the Implementation and Testing Phase (August to November 2025) involving actual development, integration, and validation; and the Documentation and Finalization Phase (November to December 2025) dedicated to report writing and project delivery.

  This timeline allocation ensures adequate time for thorough research, careful design, robust implementation, comprehensive testing, and complete documentation. The extended planning phase is justified by the complexity of integrating multiple advanced technologies including AI, AR, and BLE communication, requiring careful architectural decisions and design validation before implementation begins.

  \subsection{Phase-wise Development Schedule}

  Table~\ref{tab:detailed_timeline} presents the comprehensive project timeline with detailed activities for each development phase. The schedule follows industry-standard software engineering practices with clear milestones and deliverables for each phase.

\begin{table}[H]
    \centering
    \small
    \caption{Revised Project Development Timeline}
    \label{tab:detailed_timeline}
    \begin{tabular}{|p{4.2cm}|p{1.8cm}|p{8cm}|}
      \hline
      \textbf{Phase} & \textbf{Duration} & \textbf{Key Activities and Deliverables} \\
      \hline
      \multicolumn{3}{|c|}{\textbf{Planning and Design Phase (4.5 months)}} \\
      \hline
      Background Study & 2 weeks & Domain research and problem understanding \\
      \hline
      Problem Identification & 2 weeks & Requirement gathering and feasibility study \\
      \hline
      Solution Proposal & 2 weeks & Technology selection and architecture design \\
      \hline
      Literature Review & 3 weeks & Research survey and gap analysis \\
      \hline
      Requirements Analysis & 2 weeks & Functional/non-functional requirements \\
      \hline
      System Design & 3 weeks & Database schema, API and UI design \\
      \hline
      UML Modeling & 2 weeks & Use case, activity, sequence, class diagrams \\
      \hline
      Design Review & 2 weeks & Design validation and documentation \\
      \hline
      \multicolumn{3}{|c|}{\textbf{Implementation and Testing Phase (4 months)}} \\
      \hline
      Environment Setup & 1 week & Tools, Firebase and repository setup \\
      \hline
      Authentication Module & 2 weeks & Login/signup and Firestore integration \\
      \hline
      GPS and SOS System & 2 weeks & Location tracking and SOS workflow \\
      \hline
      Backend API & 2 weeks & FastAPI endpoints and DB operations \\
      \hline
      AI Classification & 2 weeks & Cloud AI and offline fallback \\
      \hline
      AR Guidance System & 2 weeks & ARCore overlays for CPR/AED guidance \\
      \hline
      BLE Communication & 2 weeks & Offline messaging and encryption \\
      \hline
      Notification System & 1 week & Push, SMS and email alerts \\
      \hline
      System Integration & 2 weeks & Module integration and validation \\
      \hline
      Performance Testing & 1 week & Latency testing and optimization \\
      \hline
      \multicolumn{3}{|c|}{\textbf{Documentation and Finalization Phase (2.5 weeks)}} \\
      \hline
      Report Writing & 2 weeks & Technical documentation and diagrams \\
      \hline
      Final Review & 1 week & Proofreading and formatting \\
      \hline
      Submission & 1 day & Final report submission \\
      \hline
    \end{tabular}
\end{table}

  \subsection{Gantt Chart Representation}

  Figure~\ref{fig:gantt_chart} presents a Gantt chart visualization of the project timeline showing the duration, sequencing, and overlap of various development activities. The chart provides a clear view of task dependencies and critical path activities that determine the overall project duration.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapters/gantt_chart.png}
    \caption{Gantt Chart showing project timeline from February 2025 to February 2026}
    \label{fig:gantt_chart}
  \end{figure}

  The Gantt chart illustrates several important characteristics of the project schedule. The planning phase activities (shown in blue) will execute sequentially with minimal overlap to ensure each design decision is properly informed by previous research. The implementation phase activities (shown in green) will demonstrate higher parallelism where independent modules can be developed concurrently by different team members. The testing activities (shown in orange) will overlap with later implementation tasks to enable early defect detection and continuous quality assurance. The documentation phase (shown in red) will begin while final testing is still underway to maximize efficiency and meet the submission deadline.

  % -----------------------------------------------------------------------------
 \section{Cost Estimation Using COCOMO Model}

\subsection{COCOMO Model Overview}

The Constructive Cost Model (COCOMO) developed by Barry Boehm provides a systematic framework for estimating software development effort, schedule, and cost. The ResQNow project uses the Basic COCOMO model classified as \textbf{Semi-Detached mode}, indicating moderate complexity with innovation in technology integration (AI, AR, BLE mesh) while building upon established frameworks (Flutter, Firebase).

\subsection{Project Size Estimation}

Based on functional requirements, the system size is estimated as: 
\begin{itemize}
  \item \textbf{Mobile Application Layer: } 8,000 lines of Dart code
  \item \textbf{Backend Services Layer:} 3,000 lines of Python code
  \item \textbf{AI Integration Module:} 1,500 lines
  \item \textbf{AR Guidance Components:} 2,000 lines
  \item \textbf{BLE Communication Module:} 1,500 lines
\end{itemize}

\textbf{Total Estimated Lines of Code (KLOC):} 16,000 lines = 16 KLOC

\subsection{COCOMO Effort and Schedule Calculation}

Using Basic COCOMO equations for Semi-Detached mode:

\textbf{Effort: } $E = 3. 0 \times (16)^{1.12} \approx 60.45$ Person-Months

\textbf{Development Time:} $TDEV = 2.5 \times (60.45)^{0.35} \approx 10.65$ months

\textbf{Adjusted Duration (4-person team):} $\frac{60.45}{4} \approx 15$ months (compressible to 11-12 months with agile methodology)

\textbf{Productivity: } $\frac{16}{60.45} \approx 265$ LOC per Person-Month

\subsection{Phase-wise Effort Distribution}

\begin{table}[H]
  \centering
  \caption{Phase-wise Effort, Schedule, and Cost Distribution (4-Person Team)}
  \label{tab:phase_effort}
  \begin{tabular}{|p{2.8cm}|p{2cm}|p{2cm}|p{2.5cm}|p{2.5cm}|}
    \hline
    \textbf{Phase} & \textbf{Effort (\%)} & \textbf{Effort (PM)} & \textbf{Duration (Months)} & \textbf{Cost (Rs)} \\
    \hline
    Inception & 6\% & 3.63 & 0.91 & 54,450 \\
    \hline
    Elaboration & 24\% & 14.51 & 3.63 & 2,17,650 \\
    \hline
    Construction & 56\% & 33.85 & 8.46 & 5,07,750 \\
    \hline
    Transition & 14\% & 8.46 & 2.12 & 1,26,900 \\
    \hline
    \textbf{Total} & \textbf{100\%} & \textbf{60.45} & \textbf{15. 12} & \textbf{9,06,750} \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{chapters/phase_distribution.png}
  \caption{Phase-wise distribution of effort, schedule, and cost}
  \label{fig:phase_distribution}
\end{figure}

\subsection{Activity-wise Effort Distribution}

\begin{table}[H]
  \centering
  \caption{Activity-wise Effort Distribution}
  \label{tab:activity_effort}
  \begin{tabular}{|p{5cm}|p{2.5cm}|p{3cm}|}
    \hline
    \textbf{Activity} & \textbf{Effort (\%)} & \textbf{Effort (PM)} \\
    \hline
    Management \& Planning & 10\% & 6.05 \\
    \hline
    Requirements Analysis & 18\% & 10.88 \\
    \hline
    Design \& Architecture & 22\% & 13.30 \\
    \hline
    Implementation (Coding) & 28\% & 16.93 \\
    \hline
    Testing \& Quality Assurance & 15\% & 9.07 \\
    \hline
    Documentation & 7\% & 4.23 \\
    \hline
    \textbf{Total} & \textbf{100\%} & \textbf{60.45} \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{chapters/activity_distribution.png}
  \caption{Activity-wise effort distribution across development tasks}
  \label{fig:activity_distribution}
\end{figure}

\subsection{Detailed Cost Breakdown}

\begin{table}[H]
  \centering
  \caption{Detailed Project Cost Breakdown (4-Person Team)}
  \label{tab:detailed_cost}
  \begin{tabular}{|p{4.5cm}|p{5.5cm}|p{2.5cm}|}
    \hline
    \textbf{Cost Component} & \textbf{Description} & \textbf{Amount (Rs)} \\
    \hline
    \multicolumn{3}{|c|}{\textbf{Personnel Costs}} \\
    \hline
    Development Team & 4 developers × 15 months × Rs. 15,000/PM & 9,00,000 \\
    \hline
    \multicolumn{3}{|c|}{\textbf{Infrastructure \& Services}} \\
    \hline
    Cloud Services & Firebase (Free Tier), Google Cloud & 8,000 \\
    \hline
    AI API Usage & OpenAI/Gemini API calls & 12,000 \\
    \hline
    Communication Services & Twilio SMS, SendGrid Email & 5,000 \\
    \hline
    \multicolumn{3}{|c|}{\textbf{Hardware \& Testing}} \\
    \hline
    Testing Devices & Android phones (existing devices) & 10,000 \\
    \hline
    \multicolumn{3}{|c|}{\textbf{Documentation \& Miscellaneous}} \\
    \hline
    Documentation & Report printing, materials & 3,000 \\
    \hline
    Travel \& Coordination & Team meetings & 2,000 \\
    \hline
    \multicolumn{3}{|c|}{\textbf{Contingency}} \\
    \hline
    Risk Buffer (10\%) & Unforeseen expenses & 94,000 \\
    \hline
    \multicolumn{2}{|r|}{\textbf{Total Project Cost}} & \textbf{10,34,000} \\
    \hline
    \multicolumn{2}{|r|}{\textbf{Rounded Total}} & \textbf{Rs. 10,50,000} \\
    \hline
  \end{tabular}
\end{table}

\subsection{Cost Distribution Visualization}

Figure~\ref{fig:cost_pie_chart} presents the proportional distribution of costs across major categories. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.82\textwidth]{chapters/cost_pie_chart.png}
  \caption{Percentage distribution of project costs across categories}
  \label{fig:cost_pie_chart}
\end{figure}

The cost breakdown reveals:  87\% personnel costs, 4\% infrastructure and services, 1\% hardware and testing, and 9\% contingency reserves. 

\subsection{Zero-Cost Implementation for Academic Demonstration}

\textbf{Note:} For college-level demonstration and academic project purposes, this system can be built with \textbf{zero monetary cost} by utilizing free-tier cloud services (Firebase Spark Plan, Google Cloud Free Tier), free AI API credits (OpenAI/Gemini trial quotas), personal mobile devices for testing, open-source development tools, and volunteer student development effort.  This makes the project highly feasible for educational institutions with limited budgets while still demonstrating full system capabilities. 

  \subsection{Risk Factors and Mitigation}

  Key risk factors that may affect cost estimation:

  \begin{itemize}
    \item \textbf{Technology Learning Curve:} AR and BLE expertise development may increase effort by 10-15\%
    \item \textbf{AI API Cost Variability:} Usage costs may vary by 20-30\% based on query volume
    \item \textbf{Integration Complexity:} Multi-technology integration may extend construction by 2-3 weeks
    \item \textbf{Device Compatibility:} Cross-device testing may require additional devices and optimization
    \item \textbf{Scope Changes:} Requirement modifications could increase effort by 10-20\%
  \end{itemize}

  The 15\% contingency buffer (Rs. 1,89,000) will provide risk absorption capacity. Regular sprint reviews and agile adaptation will enable proactive risk management within budget constraints.

  \subsection{Team Composition and Role Distribution}

  The 4-person team structure will be organized as follows:

  \begin{itemize}
    \item \textbf{Team Lead \& Backend Developer (1):} Project coordination, backend API development, database design, cloud infrastructure management
    \item \textbf{Mobile Developer \& UI/UX (1):} Flutter app development, UI design, state management, GPS integration
    \item \textbf{AI \& AR Specialist (1):} AI classification module, ARCore integration, 3D modeling, computer vision
    \item \textbf{BLE \& Testing Engineer (1):} Bluetooth mesh networking, offline communication, system testing, quality assurance
  \end{itemize}

  Cross-functional collaboration will ensure knowledge sharing and reduce single-point dependencies. Team members will contribute to multiple modules based on project phase requirements, enabling efficient resource utilization despite the smaller team size.

  \subsection{Conclusion}

  The COCOMO-based estimation provides quantitative foundation for planning. With a 4-person team, the project will require 60.45 person-months of effort over approximately 15 months, with an estimated total cost of Rs. 15 lakhs. The phase-wise and activity-wise breakdowns will enable effective budget management, milestone tracking, and risk mitigation. While the smaller team size extends the timeline compared to the optimal 6-person configuration, efficient task parallelization, agile methodology, and dedicated role assignments will ensure successful delivery of the complex emergency response system integrating AI, AR, and offline communication technologies.
  % -----------------------------------------------------------------------------
\section{Error Sources and Mitigation Strategies}

\textbf{GPS Location Errors:} Indoor environments and urban canyons can degrade accuracy beyond 50 meters.  Mitigated through Kalman filtering, A-GPS, Wi-Fi/cellular triangulation fallback, and flagging low-confidence readings.

\textbf{BLE Signal Variability:} Physical obstacles and electromagnetic interference cause unpredictable attenuation. Addressed via outdoor testing, multi-hop relay with retries, message persistence, and RSSI logging.

\textbf{AI Classification Ambiguity:} Vague descriptions may cause misclassification.  Handled through confidence thresholds (fallback when < 0.7), user confirmation for moderate cases, manual override options, and continuous model training.

\textbf{Network Latency Variability:} Congestion and signal fluctuations affect response times. Mitigated by multiple trial runs (n=30), timeout handling with exponential backoff, asynchronous processing, and auto-scaling infrastructure.

\textbf{Device Hardware Heterogeneity:} Variations in processor, GPU, and RAM affect performance. Addressed through testing across device tiers, adaptive quality settings, graceful degradation, and normalized metrics. 

\textbf{Timestamp Synchronization Errors:} Clock drift and timezone issues may introduce inaccuracies.  Resolved using NTP synchronization, high-precision system clocks, same-device timestamps, and UTC recording.

\section{Data Analysis Methodology}

\textbf{Descriptive Statistics:} Mean, standard deviation, min/max values, and 95\% confidence intervals will be calculated for all measured parameters.

\textbf{Comparative Analysis:} Performance will be compared using latency bar charts across network conditions, classification confusion matrices, BLE success rate plots vs. distance, and resource utilization charts.

\textbf{Statistical Validation:} Hypothesis testing will verify if $T_{dispatch} < 3000$ ms with statistical significance.  Correlation and regression analyses will examine relationships between variables and predict behavior under untested conditions. 

\textbf{Visualization: } Results will be presented using box plots, scatter plots, heat maps, and time series graphs.  All analysis will use Python (Matplotlib, Seaborn, Pandas) with raw data archived in CSV format.
  % -----------------------------------------------------------------------------
}
